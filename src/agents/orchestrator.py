from agents.schemas import ReservingInput, AgentRole, AgentLog, AnalysisType
from agents.methodology import MethodologyAgent
from agents.validation import ValidationAgent
from agents.reporting import ReportingAgent
from agents.qa import QASpecialistAgent
from agents.intelligent_selection import IntelligentSelectionAgent
from agents.code_agent import CodeAgent
from agents.llm_utils import LLMClient

import time
import json
import threading
from typing import List, Dict, Any, Optional
from dataclasses import dataclass, field
from datetime import datetime


def _run_with_timeout(func, timeout_seconds, phase_name="phase"):
    """
    Run a function with a timeout using a daemon thread.
    Returns (success: bool, error_message: str or None).
    If timeout is reached, the daemon thread is abandoned.
    """
    result = {"ok": False, "error": None}

    def target():
        try:
            func()
            result["ok"] = True
        except Exception as e:
            result["error"] = str(e)

    thread = threading.Thread(target=target, daemon=True)
    thread.start()
    thread.join(timeout=timeout_seconds)

    if thread.is_alive():
        return False, f"{phase_name} timed out after {timeout_seconds}s"
    elif result["error"]:
        return False, result["error"]
    return True, None


@dataclass
class ConversationMessage:
    """Single message in conversation history."""

    role: str  # "user" or "assistant"
    content: str
    timestamp: datetime = field(default_factory=datetime.now)
    intent: Optional[str] = None


class ConversationMemory:
    """
    Conversation buffer for context continuity.

    Enables:
    - "e per il 2022?" (references previous context)
    - "mostrami di pi√π" (continuation)
    - "perch√©?" (follow-up)
    """

    def __init__(self, max_messages: int = 20):
        self.messages: List[ConversationMessage] = []
        self.max_messages = max_messages

    def add(self, role: str, content: str, intent: str = None):
        """Add message to history."""
        self.messages.append(
            ConversationMessage(role=role, content=content, intent=intent)
        )
        # Trim if too long
        if len(self.messages) > self.max_messages:
            self.messages = self.messages[-self.max_messages :]

    def get_recent(self, n: int = 5) -> List[Dict]:
        """Get last n messages as dicts."""
        return [{"role": m.role, "content": m.content} for m in self.messages[-n:]]

    def get_context_string(self, n: int = 5) -> str:
        """Get recent conversation as string for LLM context."""
        recent = self.get_recent(n)
        if not recent:
            return "No previous conversation."

        lines = []
        for msg in recent:
            prefix = "User" if msg["role"] == "user" else "Assistant"
            lines.append(f"{prefix}: {msg['content'][:200]}")
        return "\n".join(lines)

    def clear(self):
        """Clear history."""
        self.messages = []


class Orchestrator:
    """
    ENHANCED QUESTION-DRIVEN ORCHESTRATOR

    Improvements:
    1. Multi-label intent detection (NEW_ANALYSIS, Q_AND_A, CODE_QUERY, HYBRID)
    2. Conversation memory for context continuity
    3. CodeAgent for dynamic calculations
    4. Smart routing based on query complexity
    """

    def __init__(self):
        self.role = AgentRole.ORCHESTRATOR
        self.logs = []

        # Collaborative Team
        self.methodology = MethodologyAgent()
        self.selector = IntelligentSelectionAgent()
        self.validator = ValidationAgent()
        self.reporter = ReportingAgent()
        self.qa_specialist = QASpecialistAgent()
        self.code_agent = CodeAgent()  # NEW: Dynamic code execution

        # Brain
        self.llm = LLMClient()

        # Memory
        self.memory = ConversationMemory()

        # Cache for triangle (for code agent)
        self._triangle_cache = None

    def determine_intent(self, query: str, has_context: bool) -> Dict[str, Any]:
        """
        MULTI-LABEL intent detection.

        Returns:
            {
                "primary": "NEW_ANALYSIS" | "Q_AND_A" | "CODE_QUERY",
                "needs_code": bool,  # Requires dynamic calculation
                "is_followup": bool,  # References previous conversation
                "confidence": float
            }
        """
        result = {
            "primary": "Q_AND_A",
            "needs_code": False,
            "is_followup": False,
            "confidence": 0.5,
        }

        query_lower = query.lower()

        # Check for follow-up patterns
        followup_patterns = [
            "e per",
            "e il",
            "e la",
            "e nel",
            "invece",
            "perch√©",
            "why",
            "come mai",
            "mostra",
            "show me",
            "pi√π dettagli",
            "more detail",
            "spiega",
            "explain",
            "quello",
            "questo",
            "that",
            "this",
            "the same",
        ]
        result["is_followup"] = any(p in query_lower for p in followup_patterns)

        # Check for code-requiring patterns (complex calculations)
        code_patterns = [
            "correlazione",
            "correlation",
            "covarianza",
            "covariance",
            "trend",
            "regressione",
            "regression",
            "forecast",
            "predict",
            "distribuzione",
            "distribution",
            "histogram",
            "percentile",
            "somma",
            "sum",
            "media",
            "mean",
            "average",
            "std",
            "deviazione",
            "massimo",
            "max",
            "minimo",
            "min",
            "range",
            "confronta",
            "compare",
            "differenza",
            "difference",
            "filtra",
            "filter",
            "where",
            "maggiore",
            "greater",
            "less",
            "calcola",
            "calculate",
            "compute",
            "quanto",
            "how much",
        ]
        result["needs_code"] = any(p in query_lower for p in code_patterns)

        # Check for new analysis patterns
        analysis_patterns = [
            "analizza",
            "analyze",
            "run",
            "esegui",
            "calcola riserve",
            "calculate reserves",
            "update",
            "aggiorna",
            "nuovo",
            "new analysis",
            "full analysis",
            "stress test",
            "bootstrap",
        ]

        if not self.llm.is_available():
            # Fallback heuristic
            if any(k in query_lower for k in analysis_patterns) or not has_context:
                result["primary"] = "NEW_ANALYSIS"
                result["confidence"] = 0.7
            elif result["needs_code"] and has_context:
                result["primary"] = "CODE_QUERY"
                result["confidence"] = 0.8
            else:
                result["primary"] = "Q_AND_A"
                result["confidence"] = 0.6
            return result

        # Use LLM for better classification
        conversation_context = self.memory.get_context_string(3)

        system_prompt = """You are the Brain of an Actuarial Orchestrator.
Classify the User Query into ONE of these intents:

1. 'NEW_ANALYSIS': User wants to run a new calculation or analysis (e.g., "Analyze 2024", "Run stress test")
2. 'Q_AND_A': User asks about existing results or explanations (e.g., "What is the IBNR?", "Why is it high?")
3. 'CODE_QUERY': User asks a complex question that requires dynamic calculation (e.g., "What's the correlation between factors?", "Calculate the mean reserve")

Also determine:
- needs_code: true if the answer requires computing something not pre-calculated
- is_followup: true if the query references previous conversation

Output JSON only:
{"primary": "...", "needs_code": true/false, "is_followup": true/false, "confidence": 0.0-1.0}"""

        user_prompt = f"""Recent conversation:
{conversation_context}

Current query: {query}
Has existing context: {has_context}

Classify this query."""

        try:
            response = self.llm.get_completion(system_prompt, user_prompt)

            # Parse JSON
            start = response.find("{")
            end = response.rfind("}") + 1
            if start >= 0 and end > start:
                parsed = json.loads(response[start:end])
                result.update(parsed)

            # Override: force NEW_ANALYSIS if no context and not a general question
            if not has_context and result["primary"] != "Q_AND_A":
                result["primary"] = "NEW_ANALYSIS"

        except Exception as e:
            print(f"[Orchestrator] Intent detection error: {e}")

        return result

    def route_request(
        self,
        message: str,
        current_result: dict = None,
        inputs: ReservingInput = ReservingInput(),
    ):
        """
        Unified entry point for the UI.
        Enhanced with multi-label routing and code execution.
        """
        has_context = current_result is not None

        # Save to memory
        self.memory.add("user", message)

        # 1. Decide Intent (multi-label)
        yield {
            "step": "router",
            "status": "running",
            "message": "ü§î Analyzing query...",
        }
        intent = self.determine_intent(message, has_context)

        primary = intent["primary"]
        needs_code = intent.get("needs_code", False)
        is_followup = intent.get("is_followup", False)

        # Log intent
        intent_msg = f"üëâ Intent: {primary}"
        if needs_code:
            intent_msg += " (needs code)"
        if is_followup:
            intent_msg += " (follow-up)"

        yield {"step": "router", "status": "done", "message": intent_msg}

        # 2. Route based on intent
        if primary == "NEW_ANALYSIS" or (not has_context and primary != "Q_AND_A"):
            # Run full analysis pipeline
            final_res = None
            for update in self.stream_workflow(message, inputs):
                yield update
                if update["step"] == "complete":
                    final_res = update["result"]

            # Auto-followup synthesis
            if final_res:
                yield {
                    "step": "final_synthesis",
                    "status": "running",
                    "message": "‚úçÔ∏è Synthesizing answer...",
                }

                synthesis, log = self.qa_specialist.answer_query(
                    f"Summarize the results for: {message}",
                    final_res,
                    conversation_history=self.memory.get_recent(3),
                )
                self.memory.add("assistant", synthesis, intent=primary)
                yield {"step": "final_synthesis", "status": "done", "data": synthesis}

        elif primary == "CODE_QUERY" and has_context:
            # Use CodeAgent for dynamic calculation
            for update in self.execute_code_query(message, current_result):
                yield update

        else:  # Q_AND_A
            # Standard Q&A with memory context
            # Ensure context is a dict, not None
            safe_context = current_result if current_result is not None else {}
            for update in self.ask_question(message, safe_context):
                yield update

    def execute_code_query(self, query: str, context: dict):
        """Execute dynamic code to answer complex questions."""
        yield {
            "step": "code",
            "status": "running",
            "message": "üêç Generating and executing code...",
        }

        structured = context.get("structured_results")
        if not structured:
            yield {
                "step": "code",
                "status": "done",
                "data": "No analysis results available for code execution.",
            }
            return

        # Execute code
        response, log = self.code_agent.answer_with_code(
            query, structured, triangle_df=self._triangle_cache
        )

        self.logs.append(log)
        self.memory.add("assistant", response, intent="CODE_QUERY")

        yield {"step": "code", "status": "done", "data": response, "log": log}

    def stream_workflow(self, request: str, inputs: ReservingInput = ReservingInput()):
        """Generator that yields updates for the UI."""
        import pandas as pd
        from pathlib import Path

        self.logs = []

        # 1. Methodology
        yield {
            "step": "methodology",
            "status": "running",
            "message": "Analyzing request...",
        }
        config, log1 = self.methodology.plan_analysis(request)
        self.logs.append(log1)
        yield {"step": "methodology", "status": "done", "data": config, "log": log1}

        # 2. LLM Method Selection (Intelligent) + One-Shot Validation Feedback
        yield {
            "step": "selection",
            "status": "running",
            "message": "üß† Intelligent agent is selecting optimal method...",
        }

        # Load data first (needed for both selection and execution)
        try:
            triangle = pd.read_csv(inputs.triangle_path, index_col=0)
            self._triangle_cache = triangle  # Cache for CodeAgent

            premium = None
            if inputs.premium_path:
                premium = pd.read_csv(inputs.premium_path, index_col=0).iloc[:, 0]
        except Exception as e:
            yield {
                "step": "selection",
                "status": "error",
                "message": f"Failed to load data: {e}",
            }
            return

        try:
            # Stream the thinking process
            initial_selection = None
            try:
                for thought_update in self.selector.analyze_and_select_stream(
                    triangle, verbose=True
                ):
                    # Yield the thinking process to UI
                    yield {
                        "step": f"selection_thought_{thought_update['phase']}",
                        "status": thought_update["status"],
                        "message": thought_update["content"],
                        "thought_data": thought_update.get("data"),
                    }
                    # Keep the final result
                    if thought_update["phase"] == "complete":
                        initial_selection = thought_update["data"]["result"]
            except Exception as e:
                # Fallback to non-streaming version
                yield {
                    "step": "selection_thought_fallback",
                    "status": "running",
                    "message": f"‚ö†Ô∏è Streaming failed, using standard method...",
                }
                initial_selection = self.selector.analyze_and_select(
                    triangle, verbose=True
                )

            # One-shot peer review (no loop)
            pattern_analysis = {
                "smoothing_applied": initial_selection.pattern_analysis.smoothing_applied,
                "smoothing_method": initial_selection.pattern_analysis.smoothing_method,
                "smoothing_weight": initial_selection.pattern_analysis.smoothing_weight,
            }

            feedback = self.validator.review_selection(
                selection_result={
                    "selected_estimator": initial_selection.selected_estimator,
                    "confidence": initial_selection.confidence.value
                    if initial_selection.confidence
                    else "uncertain",
                    "reasoning": (
                        initial_selection.thought_process.decision.reasoning
                        if initial_selection.thought_process
                        and initial_selection.thought_process.decision
                        else initial_selection.explanation
                    ),
                },
                estimator_results=initial_selection.all_estimator_results,
                diagnostics=initial_selection.diagnostics_summary,
                pattern_analysis=pattern_analysis,
                verbose=True,
            )

            final_selection = self.selector.consider_validation_feedback(
                initial_selection, feedback, verbose=True
            )

            selection_result = {
                "selected_estimator": final_selection.selected_estimator,
                "estimator_reason": (
                    final_selection.thought_process.decision.reasoning
                    if final_selection.thought_process
                    and final_selection.thought_process.decision
                    else final_selection.explanation
                ),
                "bf_years": [],
                "bf_reason": "",
                "summary": final_selection.explanation,
                "all_estimators": final_selection.all_estimator_results,
                "maturity_by_year": {},
                "validation_metrics": final_selection.validation_metrics.get(
                    "detailed_metrics", {}
                ),
                "pattern_analysis": pattern_analysis,
                "prudential_adjustments": final_selection.prudential_adjustments,
                "adjusted_factors": {
                    str(k): round(float(v), 4)
                    for k, v in final_selection.adjusted_factors.items()
                },
                "validation_feedback": {
                    "agrees": feedback.agrees_with_selection,
                    "concerns": feedback.concerns,
                    "suggestions": feedback.suggestions,
                    "alternative": feedback.alternative_recommendation,
                    "reasoning": feedback.reasoning,
                    "confidence": feedback.confidence.value if hasattr(feedback.confidence, 'value') else str(feedback.confidence),
                },
            }

            log_sel = AgentLog(
                agent=AgentRole.METHODOLOGY,
                action="Intelligent Method Selection",
                details=f"Selected: {selection_result.get('selected_estimator', 'Unknown')}",
            )
            self.logs.append(log_sel)

            yield {
                "step": "selection",
                "status": "done",
                "data": selection_result,
                "log": log_sel,
            }
        except Exception as e:
            selection_result = {
                "selected_estimator": "Volume-Weighted",
                "estimator_reason": "Default (selection failed)",
                "bf_years": [],
                "bf_reason": "",
                "summary": f"Using default method due to error: {e}",
            }
            yield {"step": "selection", "status": "done", "data": selection_result}

        # 3. Execution - Phase by phase with timeouts and progress streaming
        yield {
            "step": "execution",
            "status": "running",
            "message": "Initializing actuarial workflow...",
        }

        # Get pre-computed factors from selection phase
        precomputed = None
        try:
            precomputed = final_selection.adjusted_factors
        except Exception:
            pass

        # Set up workflow directly for phase-by-phase control
        from enhanced_workflow import EnhancedReservingWorkflow

        workflow = EnhancedReservingWorkflow(
            triangle=triangle, earned_premium=premium, verbose=True
        )

        if precomputed is not None:
            workflow.selected_factors = precomputed
        else:
            # Fallback: use simple average
            from model_selection.factor_estimators import SimpleAverageEstimator
            estimator = SimpleAverageEstimator()
            workflow.selected_factors = estimator.estimate(triangle)

        # Phase timeouts (seconds)
        TIMEOUT_CL = 30
        TIMEOUT_MACK = 60
        TIMEOUT_BOOTSTRAP = 120
        TIMEOUT_DIAGNOSTICS = 60
        TIMEOUT_ALT_METHODS = 60
        TIMEOUT_STRESS = 90

        execution_phases = []

        # Phase 1: Chain Ladder (always runs)
        yield {"step": "execution_phase", "status": "running",
               "message": "üìä Running Chain Ladder..."}
        ok, err = _run_with_timeout(
            lambda: workflow.run_chain_ladder(),
            TIMEOUT_CL, "Chain Ladder"
        )
        if ok:
            execution_phases.append("chain_ladder")
            yield {"step": "execution_phase", "status": "done",
                   "message": "‚úì Chain Ladder complete"}
        else:
            yield {"step": "execution_phase", "status": "warning",
                   "message": f"‚ö†Ô∏è Chain Ladder: {err}"}
            # Chain Ladder is essential - abort if it fails
            yield {"step": "execution", "status": "error",
                   "message": f"Analysis failed: Chain Ladder error: {err}"}
            return

        # Phase 2: Mack Model (skip for QUICK)
        if config.analysis_type != AnalysisType.QUICK:
            yield {"step": "execution_phase", "status": "running",
                   "message": "üìà Running Mack stochastic model..."}
            ok, err = _run_with_timeout(
                lambda: workflow.run_mack_model(),
                TIMEOUT_MACK, "Mack Model"
            )
            if ok:
                execution_phases.append("mack")
                yield {"step": "execution_phase", "status": "done",
                       "message": "‚úì Mack model complete"}
            else:
                yield {"step": "execution_phase", "status": "warning",
                       "message": f"‚ö†Ô∏è Mack model skipped: {err}"}

        # Phase 3: Bootstrap (if configured; timeout protects against infinite runs)
        if config.run_bootstrap:
            n_sims = config.n_bootstrap_simulations or 500
            yield {"step": "execution_phase", "status": "running",
                   "message": f"üé≤ Running Bootstrap ({n_sims} simulations)..."}
            ok, err = _run_with_timeout(
                lambda: workflow.run_bootstrap(n_simulations=n_sims),
                TIMEOUT_BOOTSTRAP, "Bootstrap"
            )
            if ok:
                execution_phases.append("bootstrap")
                yield {"step": "execution_phase", "status": "done",
                       "message": "‚úì Bootstrap complete"}
            else:
                yield {"step": "execution_phase", "status": "warning",
                       "message": f"‚ö†Ô∏è Bootstrap skipped: {err}"}

        # Phase 4: Alternative methods (FULL + premium only)
        if config.analysis_type == AnalysisType.FULL and premium is not None:
            yield {"step": "execution_phase", "status": "running",
                   "message": "üèñÔ∏è Running Cape Cod method..."}
            ok, err = _run_with_timeout(
                lambda: workflow.run_alternative_methods(),
                TIMEOUT_ALT_METHODS, "Alternative Methods"
            )
            if ok:
                execution_phases.append("cape_cod")
                yield {"step": "execution_phase", "status": "done",
                       "message": "‚úì Cape Cod complete"}
            else:
                yield {"step": "execution_phase", "status": "warning",
                       "message": f"‚ö†Ô∏è Cape Cod skipped: {err}"}

        # Phase 5: Diagnostics
        if config.run_diagnostics:
            yield {"step": "execution_phase", "status": "running",
                   "message": "üî¨ Running diagnostics..."}
            ok, err = _run_with_timeout(
                lambda: workflow.run_diagnostics(),
                TIMEOUT_DIAGNOSTICS, "Diagnostics"
            )
            if ok:
                execution_phases.append("diagnostics")
                yield {"step": "execution_phase", "status": "done",
                       "message": "‚úì Diagnostics complete"}
            else:
                yield {"step": "execution_phase", "status": "warning",
                       "message": f"‚ö†Ô∏è Diagnostics skipped: {err}"}

        # Phase 6: Stress Testing (FULL only)
        if config.run_stress_testing and config.analysis_type == AnalysisType.FULL:
            yield {"step": "execution_phase", "status": "running",
                   "message": "üí™ Running stress tests..."}
            ok, err = _run_with_timeout(
                lambda: workflow.run_stress_testing(),
                TIMEOUT_STRESS, "Stress Testing"
            )
            if ok:
                execution_phases.append("stress")
                yield {"step": "execution_phase", "status": "done",
                       "message": "‚úì Stress testing complete"}
            else:
                yield {"step": "execution_phase", "status": "warning",
                       "message": f"‚ö†Ô∏è Stress testing skipped: {err}"}

        # Package results
        try:
            results = self.selector._package_results(workflow, config, triangle, True)
        except Exception as e:
            yield {"step": "execution", "status": "error",
                   "message": f"Result packaging failed: {e}"}
            return

        # Attach selection results
        from agents.schemas import MethodSelection

        results.method_selection = MethodSelection(
            selected_estimator=selection_result.get("selected_estimator", "Unknown"),
            estimator_reason=selection_result.get("estimator_reason", ""),
            bf_years=selection_result.get("bf_years", []),
            bf_reason=selection_result.get("bf_reason", ""),
            summary=selection_result.get("summary", ""),
            all_estimators=selection_result.get("all_estimators", {}),
            maturity_by_year=selection_result.get("maturity_by_year", {}),
            validation_metrics=selection_result.get("validation_metrics", {}),
            prudential_adjustments=selection_result.get("prudential_adjustments", []),
            adjusted_factors=selection_result.get("adjusted_factors", {}),
        )

        log2 = AgentLog(
            agent=AgentRole.METHODOLOGY,
            action="Analysis Execution",
            details="Calculated reserves",
        )
        self.logs.append(log2)
        yield {"step": "execution", "status": "done", "data": results, "log": log2}

        # 4. Validation
        yield {
            "step": "validation",
            "status": "running",
            "message": "Peer reviewer is checking assumptions...",
        }
        validation_report, log3 = self.validator.validate(results)
        self.logs.append(log3)
        yield {
            "step": "validation",
            "status": "done",
            "data": validation_report,
            "log": log3,
        }

        # 5. Reporting
        yield {
            "step": "reporting",
            "status": "running",
            "message": "Drafting final report...",
        }
        final_report, log4 = self.reporter.generate_report(results, validation_report)
        self.logs.append(log4)
        yield {"step": "reporting", "status": "done", "data": final_report, "log": log4}

        # Final Bundle
        yield {
            "step": "complete",
            "result": {
                "report": final_report,
                "structured_results": results,
                "validation": validation_report,
                "method_selection": selection_result,
                "audit_trail": self.logs,
            },
        }

    def ask_question(self, query: str, context: dict):
        """Enhanced Q&A with conversation memory."""
        yield {"step": "qa", "status": "running", "message": "Consulting results..."}

        # Pass conversation history for context
        response, log = self.qa_specialist.answer_query(
            query, context, conversation_history=self.memory.get_recent(5)
        )

        self.logs.append(log)
        self.memory.add("assistant", response, intent="Q_AND_A")

        yield {"step": "qa", "status": "done", "data": response, "log": log}

    def clear_memory(self):
        """Clear conversation history."""
        self.memory.clear()
        print("[Orchestrator] Conversation memory cleared.")
